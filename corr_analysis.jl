


include("const.jl")
include("utils.jl")
include("feature_generator/feature_generator.jl")

using LinearAlgebra
using Statistics
using StatsBase
using Plots
using Printf

date_list_train = [
    # [string(di) for di in 20230313:20230331]; 
    [string(di) for di in 20230401:20230430];
    [string(di) for di in 20230501:20230531]
    ]

df_list = [get_df_oneday_full(tardis_dir, s7_dir, symbol, date_one_day, features) for date_one_day in date_list_train]
df_train = vcat(df_list...)
set_ret_bp(df_train, ret_interval)


date_list_test = [
    [string(di) for di in 20230525:20230531];
    [string(di) for di in 20230601:20230630]
    ]

df_list2 = [get_df_oneday_full(tardis_dir, s7_dir, symbol, date_one_day, features) for date_one_day in date_list_test]
df_test = vcat(df_list2...)
set_ret_bp(df_test, ret_interval)




ret_col_name = "ret_bp"
th_vec = [(0.1:0.1:0.9); Float64.(1:99); (99.1:0.1:99.9)]

println("--------------------------------------------\n")
ft_gen_map = Dict(
    "ofi" => ft_gen_ofi,
    "aq" => ft_gen_aq,
    "tor" => ft_gen_tor,
    "tv" => ft_gen_tv,
    "index_dist" => ft_gen_index_dist,
    "mark_dist" => ft_gen_mark_dist,
    "trade_cnt_ratio" => ft_gen_trcr,
    # "trade_vol" => ft_gen_trv,
    "liquidation" => ft_gen_liqu,
    "vwap" => ft_gen_vwap,
)
ft_names = collect(keys(ft_gen_map))

df_evv_train, df_evv_test = get_evv_df_fast(ft_gen_map, ret_col_name, th_vec)
for th in [0.025, 0.05, 0.1, 0.5, 1.0, 5.0]
    summary_evv_result(df_evv_train, df_evv_test, ft_names, th)
end



# df_evv_train

# df_evv_test


# ========================================
# í”¼ì²˜ ì§êµì„± ë¶„ì„ í•¨ìˆ˜ë“¤
# ========================================

begin
"""
í”¼ì–´ìŠ¨ ìƒê´€ê´€ê³„ í–‰ë ¬ì„ ê³„ì‚°í•˜ê³  ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜
"""
function analyze_feature_correlation(df_evv_train, df_evv_test, ft_names; title_suffix="")
    println("\n" * "="^60)
    println("Feature Correlation Analysis $(title_suffix)")
    println("="^60)
    
    # Train ë°ì´í„° ìƒê´€ê´€ê³„
    train_data = Matrix(df_evv_train[!, ft_names])
    # NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±°
    valid_rows = .!any(isnan.(train_data), dims=2)[:]
    train_clean = train_data[valid_rows, :]
    
    corr_train = cor(train_clean)
    
    # Test ë°ì´í„° ìƒê´€ê´€ê³„  
    test_data = Matrix(df_evv_test[!, ft_names])
    valid_rows_test = .!any(isnan.(test_data), dims=2)[:]
    test_clean = test_data[valid_rows_test, :]
    
    corr_test = cor(test_clean)
    
    # ìƒê´€ê´€ê³„ í†µê³„ ì¶œë ¥
    println("Train ë°ì´í„°:")
    print_correlation_stats(corr_train, ft_names)
    println("\nTest ë°ì´í„°:")
    print_correlation_stats(corr_test, ft_names)
    
    # íˆíŠ¸ë§µ ì‹œê°í™”
    p1 = heatmap(corr_train, 
                xticks=(1:length(ft_names), ft_names), 
                yticks=(1:length(ft_names), ft_names),
                title="Train Correlation Matrix",
                color=:RdBu,
                clims=(-1, 1))
    
    p2 = heatmap(corr_test, 
                xticks=(1:length(ft_names), ft_names), 
                yticks=(1:length(ft_names), ft_names),
                title="Test Correlation Matrix", 
                color=:RdBu,
                clims=(-1, 1))
    
    plt_corr = plot(p1, p2, layout=(1, 2), size=(1200, 500))
    display(plt_corr)
    
    return corr_train, corr_test, plt_corr
end

"""
ìƒê´€ê´€ê³„ í†µê³„ ì¶œë ¥ í•¨ìˆ˜
"""
function print_correlation_stats(corr_matrix, ft_names)
    n = size(corr_matrix, 1)
    
    # ëŒ€ê°ì„  ì œì™¸í•œ ìƒê´€ê³„ìˆ˜ë“¤ ì¶”ì¶œ
    off_diag_corrs = []
    for i in 1:n
        for j in (i+1):n
            push!(off_diag_corrs, abs(corr_matrix[i, j]))
        end
    end
    
    println(@sprintf("  í‰ê·  ì ˆëŒ€ ìƒê´€ê³„ìˆ˜: %.4f", mean(off_diag_corrs)))
    println(@sprintf("  ìµœëŒ€ ì ˆëŒ€ ìƒê´€ê³„ìˆ˜: %.4f", maximum(off_diag_corrs)))
    println(@sprintf("  ìƒê´€ê³„ìˆ˜ > 0.5ì¸ ìŒ: %dê°œ (%.1f%%)", 
             sum(off_diag_corrs .> 0.5), 
             100 * sum(off_diag_corrs .> 0.5) / length(off_diag_corrs)))
    println(@sprintf("  ìƒê´€ê³„ìˆ˜ > 0.7ì¸ ìŒ: %dê°œ (%.1f%%)", 
             sum(off_diag_corrs .> 0.7), 
             100 * sum(off_diag_corrs .> 0.7) / length(off_diag_corrs)))
    
    # ë†’ì€ ìƒê´€ê´€ê³„ ìŒë“¤ ì¶œë ¥
    high_corr_pairs = []
    for i in 1:n
        for j in (i+1):n
            if abs(corr_matrix[i, j]) > 0.5
                push!(high_corr_pairs, (ft_names[i], ft_names[j], corr_matrix[i, j]))
            end
        end
    end
    
    if !isempty(high_corr_pairs)
        println("  ë†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.5):")
        for (ft1, ft2, corr_val) in sort(high_corr_pairs, by=x->abs(x[3]), rev=true)
            println(@sprintf("    %s - %s: %.4f", ft1, ft2, corr_val))
        end
    end
end

"""
PCA ë¶„ì„ í•¨ìˆ˜
"""
function analyze_feature_pca(df_evv_train, df_evv_test, ft_names; title_suffix="")
    println("\n" * "="^60)
    println("Principal Component Analysis (PCA) $(title_suffix)")
    println("="^60)
    
    # Train ë°ì´í„° ì¤€ë¹„
    train_data = Matrix(df_evv_train[!, ft_names])
    valid_rows = .!any(isnan.(train_data), dims=2)[:]
    train_clean = train_data[valid_rows, :]
    
    # ë°ì´í„° í‘œì¤€í™”
    train_std = StatsBase.standardize(StatsBase.ZScoreTransform, train_clean, dims=1)
    
    # PCA ìˆ˜í–‰ (SVD ì‚¬ìš©)
    U, S, Vt = svd(train_std)
    
    # ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨ ê³„ì‚°
    explained_variance_ratio = (S .^ 2) ./ sum(S .^ 2)
    cumulative_variance = cumsum(explained_variance_ratio)
    
    # ê²°ê³¼ ì¶œë ¥
    println("ì£¼ì„±ë¶„ë³„ ì„¤ëª…ëœ ë¶„ì‚°:")
    for i in 1:min(length(S), 10)  # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ì¶œë ¥
        println(@sprintf("  PC%d: %.4f (ëˆ„ì : %.4f)", 
                i, explained_variance_ratio[i], cumulative_variance[i]))
    end
    
    # 90% ë¶„ì‚°ì„ ì„¤ëª…í•˜ëŠ” ì£¼ì„±ë¶„ ê°œìˆ˜
    n_components_90 = findfirst(cumulative_variance .>= 0.9)
    n_components_95 = findfirst(cumulative_variance .>= 0.95)
    println(@sprintf("\n90%% ë¶„ì‚° ì„¤ëª…ì— í•„ìš”í•œ ì£¼ì„±ë¶„: %dê°œ", n_components_90))
    println(@sprintf("95%% ë¶„ì‚° ì„¤ëª…ì— í•„ìš”í•œ ì£¼ì„±ë¶„: %dê°œ", n_components_95))
    
    # ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì˜ feature loading ì¶œë ¥
    println("\nì²« ë²ˆì§¸ ì£¼ì„±ë¶„ (PC1) ë¡œë”©:")
    pc1_loadings = Vt[1, :]
    for (i, ft_name) in enumerate(ft_names)
        println(@sprintf("  %s: %.4f", ft_name, pc1_loadings[i]))
    end
    
    # ì‹œê°í™”
    p1 = plot(1:min(length(explained_variance_ratio), 15), 
              explained_variance_ratio[1:min(length(explained_variance_ratio), 15)], 
              seriestype=:bar, 
              title="Explained Variance Ratio", 
              xlabel="Principal Component", 
              ylabel="Variance Ratio",
              legend=false)
    
    p2 = plot(1:min(length(cumulative_variance), 15), 
              cumulative_variance[1:min(length(cumulative_variance), 15)], 
              title="Cumulative Explained Variance", 
              xlabel="Principal Component", 
              ylabel="Cumulative Variance Ratio",
              legend=false,
              linewidth=2)
    hline!(p2, [0.9, 0.95], linestyle=:dash, color=:red, alpha=0.5)
    
    # Feature loading íˆíŠ¸ë§µ (ì²« 5ê°œ ì£¼ì„±ë¶„)
    n_show = min(5, size(Vt, 1))
    p3 = heatmap(Vt[1:n_show, :], 
                xticks=(1:length(ft_names), ft_names), 
                yticks=(1:n_show, ["PC$i" for i in 1:n_show]),
                title="Principal Component Loadings",
                color=:RdBu,
                clims=(-1, 1))
    
    plt_pca = plot(p1, p2, p3, layout=(2, 2), size=(1200, 800))
    display(plt_pca)
    
    return explained_variance_ratio, cumulative_variance, Vt, plt_pca
end

"""
ì§êµì„± ì¢…í•© ë¶„ì„ í•¨ìˆ˜
"""
function comprehensive_orthogonality_analysis(df_evv_train, df_evv_test, ft_names)
    println("\n" * "="^80)
    println("í”¼ì²˜ ì§êµì„± ì¢…í•© ë¶„ì„")
    println("="^80)
    
    # 1. ìƒê´€ê´€ê³„ ë¶„ì„
    corr_train, corr_test, plt_corr = analyze_feature_correlation(df_evv_train, df_evv_test, ft_names)
    
    # 2. PCA ë¶„ì„
    explained_var, cumulative_var, loadings, plt_pca = analyze_feature_pca(df_evv_train, df_evv_test, ft_names)
    
    # 3. ì§êµì„± ì ìˆ˜ ê³„ì‚°
    println("\n" * "="^60)
    println("ì§êµì„± ì¢…í•© ì ìˆ˜")
    println("="^60)
    
    # Train ë°ì´í„° ê¸°ì¤€ ì§êµì„± ì ìˆ˜
    n = length(ft_names)
    off_diag_corrs = []
    for i in 1:n
        for j in (i+1):n
            push!(off_diag_corrs, abs(corr_train[i, j]))
        end
    end
    
    orthogonality_score = 1.0 - mean(off_diag_corrs)  # í‰ê·  ì ˆëŒ€ ìƒê´€ê³„ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ì§êµì„± ë†’ìŒ
    pca_score = 1.0 - explained_var[1]  # ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ ë¶„ì‚°ì´ ì˜ ë¶„ì‚°ë¨
    
    println(@sprintf("í‰ê·  ì ˆëŒ€ ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ ì§êµì„± ì ìˆ˜: %.4f (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì§êµ)", orthogonality_score))
    println(@sprintf("PCA ê¸°ë°˜ ë¶„ì‚° ë¶„ì‚°ë„ ì ìˆ˜: %.4f (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¶„ì‚°ì´ ê³ ë¥´ê²Œ ë¶„í¬)", pca_score))
    
    # 4. ê¶Œì¥ì‚¬í•­ ì¶œë ¥
    println("\nê¶Œì¥ì‚¬í•­:")
    if mean(off_diag_corrs) > 0.3
        println("âš ï¸  í”¼ì²˜ë“¤ ê°„ ìƒê´€ê´€ê³„ê°€ ë†’ìŠµë‹ˆë‹¤. ì°¨ì› ì¶•ì†Œë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.")
    else
        println("âœ… í”¼ì²˜ë“¤ì´ ì ì ˆíˆ ë…ë¦½ì ì…ë‹ˆë‹¤.")
    end
    
    if explained_var[1] > 0.4
        println("âš ï¸  ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì´ ë¶„ì‚°ì˜ 40% ì´ìƒì„ ì„¤ëª…í•©ë‹ˆë‹¤. í”¼ì²˜ ë‹¤ì–‘ì„±ì„ ëŠ˜ë ¤ë³´ì„¸ìš”.")
    else
        println("âœ… ë¶„ì‚°ì´ ì£¼ì„±ë¶„ë“¤ì— ê³ ë¥´ê²Œ ë¶„í¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    end
    
    n_components_95 = findfirst(cumulative_var .>= 0.95)
    efficiency = n_components_95 / length(ft_names)
    if efficiency < 0.7
        println(@sprintf("âœ… ì°¨ì› íš¨ìœ¨ì„±ì´ ì¢‹ìŠµë‹ˆë‹¤. %dê°œ í”¼ì²˜ë¡œ 95%% ë¶„ì‚° ì„¤ëª… ê°€ëŠ¥ (íš¨ìœ¨ì„±: %.2f)", n_components_95, efficiency))
    else
        println(@sprintf("âš ï¸  ì°¨ì› íš¨ìœ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. %dê°œ í”¼ì²˜ê°€ 95%% ë¶„ì‚° ì„¤ëª…ì— í•„ìš” (íš¨ìœ¨ì„±: %.2f)", n_components_95, efficiency))
    end
    
    return Dict(
        "correlation_matrices" => (corr_train, corr_test),
        "pca_results" => (explained_var, cumulative_var, loadings),
        "orthogonality_score" => orthogonality_score,
        "pca_score" => pca_score,
        "plots" => (plt_corr, plt_pca)
    )
end


# ========================================
# ì§êµì„± ë¶„ì„ ì‹¤í–‰
# ========================================

"""
í”¼ì²˜ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ (ìœ ì‚¬í•œ í”¼ì²˜ë“¤ ê·¸ë£¹í•‘)
"""
function analyze_feature_clustering(corr_matrix, ft_names; linkage_method=:ward, n_clusters=3)
    println("\n" * "="^60)
    println("í”¼ì²˜ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„")
    println("="^60)
    
    # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚° (1 - |correlation|)
    dist_matrix = 1.0 .- abs.(corr_matrix)
    
    # ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (ê°„ë‹¨í•œ êµ¬í˜„)
    n = length(ft_names)
    clusters = collect(1:n)  # ê° í”¼ì²˜ë¥¼ ë³„ë„ í´ëŸ¬ìŠ¤í„°ë¡œ ì‹œì‘
    
    # ê°€ì¥ ê°€ê¹Œìš´ í”¼ì²˜ë“¤ ì°¾ê¸°
    distances = []
    for i in 1:n
        for j in (i+1):n
            push!(distances, (dist_matrix[i, j], i, j))
        end
    end
    sort!(distances)
    
    println("í”¼ì²˜ ê°„ ê±°ë¦¬ (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬):")
    for (dist, i, j) in distances[1:min(10, length(distances))]
        println(@sprintf("  %s - %s: %.4f (ìƒê´€ê³„ìˆ˜: %.4f)", 
                ft_names[i], ft_names[j], dist, corr_matrix[i, j]))
    end
    
    # ê°€ì¥ ìœ ì‚¬í•œ í”¼ì²˜ ìŒë“¤ ì‹ë³„
    similar_pairs = [(dist, i, j) for (dist, i, j) in distances if dist < 0.5]  # ìƒê´€ê³„ìˆ˜ 0.5 ì´ìƒ
    
    if !isempty(similar_pairs)
        println("\nì¤‘ë³µì„±ì´ ë†’ì€ í”¼ì²˜ ìŒë“¤ (ê±°ë¦¬ < 0.5, |ìƒê´€ê³„ìˆ˜| > 0.5):")
        for (dist, i, j) in similar_pairs
            println(@sprintf("  %s â†” %s (ê±°ë¦¬: %.3f, ìƒê´€: %.3f)", 
                    ft_names[i], ft_names[j], dist, corr_matrix[i, j]))
        end
    else
        println("\nâœ… ì¤‘ë³µì„±ì´ ë†’ì€ í”¼ì²˜ ìŒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    end
    
    return similar_pairs
end

"""
í”¼ì²˜ ì„ íƒ ê¶Œì¥ í•¨ìˆ˜
"""
function recommend_feature_selection(corr_matrix, explained_var, ft_names; 
                                   corr_threshold=0.7, pca_importance_threshold=0.1)
    println("\n" * "="^60)
    println("í”¼ì²˜ ì„ íƒ ê¶Œì¥ì‚¬í•­")
    println("="^60)
    
    n = length(ft_names)
    
    # 1. ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ í”¼ì²˜ë“¤ ì‹ë³„
    high_corr_pairs = []
    for i in 1:n
        for j in (i+1):n
            if abs(corr_matrix[i, j]) > corr_threshold
                push!(high_corr_pairs, (i, j, corr_matrix[i, j]))
            end
        end
    end
    
    # 2. PCA ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì—ì„œ ì¤‘ìš”ë„ê°€ ë‚®ì€ í”¼ì²˜ë“¤ ì‹ë³„
    pc1_loadings = abs.(explained_var)  # ì´ë¯¸ loadingsê°€ ì•„ë‹ˆë¼ explained variance ratio
    
    println("í”¼ì²˜ ì¤‘ë³µì„± ë¶„ì„:")
    if !isempty(high_corr_pairs)
        println("âš ï¸  ë†’ì€ ìƒê´€ê´€ê³„ í”¼ì²˜ ìŒë“¤:")
        for (i, j, corr_val) in high_corr_pairs
            println(@sprintf("  %s - %s: %.4f", ft_names[i], ft_names[j], corr_val))
        end
        
        # ì¤‘ë³µ í”¼ì²˜ ì œê±° ê¶Œì¥
        redundant_features = Set()
        for (i, j, corr_val) in high_corr_pairs
            # ë” ì ì€ ì •ë³´ë¥¼ ê°€ì§„ í”¼ì²˜ë¥¼ ì œê±° í›„ë³´ë¡œ ì„ íƒ (ì„ì˜ë¡œ j ì„ íƒ)
            push!(redundant_features, j)
        end
        
        if !isempty(redundant_features)
            println("\nì œê±° ê³ ë ¤ ëŒ€ìƒ í”¼ì²˜ë“¤:")
            for idx in redundant_features
                println(@sprintf("  - %s", ft_names[idx]))
            end
        end
    else
        println("âœ… ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ í”¼ì²˜ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
    end
    
    # 3. ìµœì¢… ê¶Œì¥ í”¼ì²˜ ì„¸íŠ¸
    recommended_features = setdiff(1:n, Set(j for (i, j, _) in high_corr_pairs))
    
    println("\nì¶”ì²œ í”¼ì²˜ ì„¸íŠ¸:")
    for idx in recommended_features
        println(@sprintf("  âœ“ %s", ft_names[idx]))
    end
    
    println(@sprintf("\nìš”ì•½: %dê°œ í”¼ì²˜ ì¤‘ %dê°œ ì¶”ì²œ (%.1f%% ìœ ì§€)", 
            n, length(recommended_features), 100 * length(recommended_features) / n))
    
    return collect(recommended_features), [ft_names[i] for i in recommended_features]
end

"""
í”¼ì²˜ ì¤‘ìš”ë„ vs ì§êµì„± íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„
"""
function analyze_importance_orthogonality_tradeoff(df_evv_train, df_evv_test, ft_names, 
                                                  ret_col_name="ret_bp"; test_threshold=5.0)
    println("\n" * "="^60)
    println("í”¼ì²˜ ì¤‘ìš”ë„ vs ì§êµì„± íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„")
    println("="^60)
    
    # ê° í”¼ì²˜ì˜ ê°œë³„ ì„±ëŠ¥ ì¸¡ì •
    individual_performances = []
    for ft_name in ft_names
        # ê°„ë‹¨í•œ ì„±ëŠ¥ ì¸¡ì •: í•´ë‹¹ í”¼ì²˜ë§Œ ì‚¬ìš©í–ˆì„ ë•Œ ì‹ í˜¸ ì„±ëŠ¥
        sl_train, ss_train = get_signals(df_evv_train[!, ft_name], test_threshold)
        plt_train, tr_res_vec_train = backtest_sica_2(sl_train, ss_train, 
                                                     df_train.WAP_Lag_200ms, 
                                                     df_train.WAP_Lag_0ms, 
                                                     df_train.timestamp, is_print=false)
        
        if !isempty(tr_res_vec_train)
            avg_return = mean([10_000 * tr[6] / tr[4] for tr in tr_res_vec_train])
            push!(individual_performances, (ft_name, avg_return, length(tr_res_vec_train)))
        else
            push!(individual_performances, (ft_name, 0.0, 0))
        end
    end
    
    # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬
    sort!(individual_performances, by=x->x[2], rev=true)
    
    println("ê°œë³„ í”¼ì²˜ ì„±ëŠ¥ (í‰ê·  ìˆ˜ìµë¥  bp):")
    for (ft_name, avg_ret, n_trades) in individual_performances
        println(@sprintf("  %s: %.3f bp (%d trades)", ft_name, avg_ret, n_trades))
    end
    
    # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
    train_data = Matrix(df_evv_train[!, ft_names])
    valid_rows = .!any(isnan.(train_data), dims=2)[:]
    train_clean = train_data[valid_rows, :]
    corr_matrix = cor(train_clean)
    
    # ê° í”¼ì²˜ì˜ í‰ê·  ì ˆëŒ€ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    avg_abs_corrs = []
    for (i, ft_name) in enumerate(ft_names)
        other_corrs = [abs(corr_matrix[i, j]) for j in 1:length(ft_names) if i != j]
        push!(avg_abs_corrs, (ft_name, mean(other_corrs)))
    end
    sort!(avg_abs_corrs, by=x->x[2])
    
    println("\ní”¼ì²˜ë³„ í‰ê·  ì ˆëŒ€ ìƒê´€ê³„ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ë…ë¦½ì ):")
    for (ft_name, avg_corr) in avg_abs_corrs
        println(@sprintf("  %s: %.4f", ft_name, avg_corr))
    end
    
    # ì„±ëŠ¥-ë…ë¦½ì„± ìŠ¤ì½”ì–´ ê³„ì‚°
    performance_dict = Dict(perf[1] => perf[2] for perf in individual_performances)
    independence_dict = Dict(corr[1] => 1.0 - corr[2] for corr in avg_abs_corrs)  # 1ì—ì„œ ë¹¼ì„œ ë…ë¦½ì„± ì ìˆ˜ë¡œ ë³€í™˜
    
    combined_scores = []
    for ft_name in ft_names
        perf_score = performance_dict[ft_name]
        indep_score = independence_dict[ft_name]
        # ì •ê·œí™” (ì„±ëŠ¥ì€ í‰ê· ì„ ì¤‘ì‹¬ìœ¼ë¡œ, ë…ë¦½ì„±ì€ 0-1 ìŠ¤ì¼€ì¼)
        normalized_perf = perf_score / (abs(mean([p[2] for p in individual_performances])) + 1e-8)
        combined_score = 0.6 * normalized_perf + 0.4 * indep_score  # ì„±ëŠ¥ 60%, ë…ë¦½ì„± 40% ê°€ì¤‘ì¹˜
        push!(combined_scores, (ft_name, combined_score, normalized_perf, indep_score))
    end
    
    sort!(combined_scores, by=x->x[2], rev=true)
    
    println("\nì¢…í•© ì ìˆ˜ (ì„±ëŠ¥ 60% + ë…ë¦½ì„± 40%):")
    for (ft_name, combined, perf_norm, indep) in combined_scores
        println(@sprintf("  %s: %.4f (ì„±ëŠ¥: %.4f, ë…ë¦½ì„±: %.4f)", 
                ft_name, combined, perf_norm, indep))
    end
    
    return individual_performances, avg_abs_corrs, combined_scores
end

end  # í•¨ìˆ˜ ì •ì˜ ë¸”ë¡ ë


begin  # ë¶„ì„ ì‹¤í–‰ ë¸”ë¡
println("\nğŸ” í”¼ì²˜ ì§êµì„± ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
orthogonality_results = comprehensive_orthogonality_analysis(df_evv_train, df_evv_test, ft_names)

# ì¶”ê°€ ë¶„ì„ë“¤
println("\nğŸ“Š ì¶”ê°€ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤...")

# í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
corr_train = orthogonality_results["correlation_matrices"][1]
similar_pairs = analyze_feature_clustering(corr_train, ft_names)

# í”¼ì²˜ ì„ íƒ ê¶Œì¥
recommended_idx, recommended_names = recommend_feature_selection(corr_train, 
                                                               orthogonality_results["pca_results"][1], 
                                                               ft_names)

# ì¤‘ìš”ë„-ì§êµì„± íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„  
individual_perf, avg_corrs, combined_scores = analyze_importance_orthogonality_tradeoff(
    df_evv_train, df_evv_test, ft_names)

println("\n" * "="^80)
println("ğŸ¯ ìµœì¢… ë¶„ì„ ìš”ì•½")
println("="^80)
println(@sprintf("ì „ì²´ í”¼ì²˜ ìˆ˜: %dê°œ", length(ft_names)))
println(@sprintf("ì§êµì„± ì ìˆ˜: %.4f", orthogonality_results["orthogonality_score"]))
println(@sprintf("PCA ë¶„ì‚° ë¶„ì‚°ë„: %.4f", orthogonality_results["pca_score"]))
println(@sprintf("ê¶Œì¥ í”¼ì²˜ ìˆ˜: %dê°œ", length(recommended_names)))
println("\nê¶Œì¥ í”¼ì²˜ ëª©ë¡:")
for ft_name in recommended_names
    println("  âœ“ $ft_name")
end
end  # ë¶„ì„ ì‹¤í–‰ ë¸”ë¡ ë



