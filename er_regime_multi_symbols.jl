

include("const.jl")
include("utils.jl")
include("feature_generator/feature_generator.jl")

begin
    df_train_map::Dict{String, DataFrame} = Dict()
    @threads for sb in symbols
        date_list_train = [
            # [string(di) for di in 20230313:20230331]; 
            [string(di) for di in 20230401:20230430];
            [string(di) for di in 20230501:20230531]
            ]
        # df_list = [get_df_oneday_full(tardis_dir, s7_dir, sb, date_one_day, features) for date_one_day in date_list_train]
        df_list = [Threads.@spawn get_df_oneday_full(tardis_dir, s7_dir, sb, date, features) for date in date_list_train]
        df_list = fetch.(df_list)

        df_train = vcat(df_list...)
        set_ret_bp(df_train, ret_interval)
        df_train_map[sb] = df_train
    end
end

begin
    df_test_map::Dict{String, DataFrame} = Dict()
    @threads for sb in symbols
        date_list_test = [
            [string(di) for di in 20230525:20230531];
            [string(di) for di in 20230601:20230630]
            ]

        # df_list2 = [get_df_oneday_full(tardis_dir, s7_dir, sb, date_one_day, features) for date_one_day in date_list_test]
        df_list2 = [Threads.@spawn get_df_oneday_full(tardis_dir, s7_dir, sb, date, features) for date in date_list_test]
        df_list2 = fetch.(df_list2)

        df_test = vcat(df_list2...)
        set_ret_bp(df_test, ret_interval)
        df_test_map[sb] = df_test
    end
end



############################################################

thv = [0.05, 0.1, 0.33333, 0.5, 1.0]

all_symbol_plot_train = Dict(th => Dict() for th in thv)
all_symbol_plot_test = Dict(th => Dict() for th in thv)

for target_symbol in symbols
println("target_symbol: $(target_symbol)")
begin
    # target_symbol = "BTCUSDT"
    # target_symbol = "ADAUSDT"
    win = 300
    ret_col_name = "ret_bp"
    # th_vec = [(0.1:0.1:0.9); Float64.(1:99); (99.1:0.1:99.9)]    
    th_vec = [0.01, 0.03, 0.06, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.94, 0.97, 0.99] .* 100
end

begin    
    println("--------------------------------------------\n")
    ft_gen_map = Dict(
        "ofi" => ft_gen_ofi,
        "aq" => ft_gen_aq,
        "tor" => ft_gen_tor,
        "tv" => ft_gen_tv,
        "index_dist" => ft_gen_index_dist,
        "mark_dist" => ft_gen_mark_dist,
        "liquidation" => ft_gen_liqu,
        "vwap" => ft_gen_vwap,
    )
    ft_names = collect(keys(ft_gen_map))
    regime_ft_gen_map = Dict("ttc" => (ft_gen_ttc, [0.4, 0.8]), "doi" => (ft_gen_doi, [0.1, 0.5, 0.9]))

    df_evv_train_map::Dict{String, DataFrame} = Dict()
    df_evv_test_map::Dict{String, DataFrame} = Dict()
    for sb in symbols
        println(sb)
        df_evv_train, df_evv_test = get_evv_df_fast(df_train_map[sb], df_test_map[sb], ft_gen_map, df_train_map[target_symbol].ret_bp, th_vec)
        # df_evv_train, df_evv_test = get_evv_df_fast2(df_train_map[sb], df_test_map[sb], ft_gen_map, df_train_map[target_symbol].ret_bp, th_vec, is_calc_lr=false)
        # rg_info_map = get_rg_info_map(regime_ft_gen_map, df_train_map[sb], df_test_map[sb], df_evv_train)
        # df_evv_train[!, "regime_weight_sum"] = calc_regime_based_er_vec(rg_info_map, df_evv_train, "mask_train")
        # df_evv_test[!, "regime_weight_sum"] = calc_regime_based_er_vec(rg_info_map, df_evv_test, "mask_test")

        df_evv_train_map[sb] = df_evv_train
        df_evv_test_map[sb] = df_evv_test
    end
end




ft_names = [n for n in names(df_evv_train_map["BTCUSDT"]) if !occursin("_weight_sum", n)]
df_evv_train_multi_sb = DataFrame(timestamp=df_train_map["BTCUSDT"].timestamp)
df_evv_test_multi_sb = DataFrame(timestamp=df_test_map["BTCUSDT"].timestamp)

for sb in symbols
    for ft_name in ft_names
        df_evv_train_multi_sb[!, "$(sb)__$(ft_name)"] = df_evv_train_map[sb][!, ft_name]
        df_evv_test_multi_sb[!, "$(sb)__$(ft_name)"] = df_evv_test_map[sb][!, ft_name]
    end
end



df_evv_train_multi_sb[!, "multi_equal_weight_sum"] = get_multi_equal_weight_sum(df_evv_train_multi_sb)
df_evv_test_multi_sb[!, "multi_equal_weight_sum"] = get_multi_equal_weight_sum(df_evv_test_multi_sb)

ft_cols = [ft_name for ft_name in names(df_evv_train_multi_sb) if !occursin("weight_sum", ft_name) && ft_name !== "timestamp"]
lrws_train, lrws_test = calc_weighted_sum_return(df_train_map[target_symbol], df_evv_train_multi_sb[!, ft_cols], df_evv_test_multi_sb[!, ft_cols])
df_evv_train_multi_sb[!, "multi_lin_reg_weight_sum"] = lrws_train
df_evv_test_multi_sb[!, "multi_lin_reg_weight_sum"] = lrws_test

rg_info_map = get_rg_info_map(regime_ft_gen_map, df_train_map[target_symbol], df_test_map[target_symbol], df_evv_train_multi_sb)
df_evv_train_multi_sb[!, "multi_regime_weight_sum"] = calc_multi_regime_based_er_vec(rg_info_map, df_evv_train_multi_sb, "mask_train")
df_evv_test_multi_sb[!, "multi_regime_weight_sum"] = calc_multi_regime_based_er_vec(rg_info_map, df_evv_test_multi_sb, "mask_test")




for th in thv
    begin
        println("Train")
        println("----------")
        sl_train, ss_train = get_signals(df_evv_train_multi_sb[!, "multi_equal_weight_sum"], th)
        plt_train_meq, tr_res_vec_train_meq = backtest_sica_2(sl_train, ss_train, df_train_map[target_symbol].WAP_Lag_200ms, df_train_map[target_symbol].WAP_Lag_0ms, df_train_map[target_symbol].timestamp, is_display=false, name=target_symbol)
        
        println("----------")
        sl_train, ss_train = get_signals(df_evv_train_multi_sb[!, "multi_lin_reg_weight_sum"], th)
        plt_train_mlr, tr_res_vec_train_mlr = backtest_sica_2(sl_train, ss_train, df_train_map[target_symbol].WAP_Lag_200ms, df_train_map[target_symbol].WAP_Lag_0ms, df_train_map[target_symbol].timestamp, is_display=false, name=target_symbol)
        
        println("----------")
        sl_train, ss_train = get_signals(df_evv_train_multi_sb[!, "multi_regime_weight_sum"], th)
        plt_train_mrg, tr_res_vec_train_mrg = backtest_sica_2(sl_train, ss_train, df_train_map[target_symbol].WAP_Lag_200ms, df_train_map[target_symbol].WAP_Lag_0ms, df_train_map[target_symbol].timestamp, is_display=false, name=target_symbol)
              
        # final_plt_train = plot(plt_train_meq, plt_train_mlr, plt_train_mrg, layout=(3, 1), plot_title="[$(target_symbol)] Multi Symbol Train - th: $th", size=(900, 900))        
        # savefig(final_plt_train, "./fig/multi_symbol_BT_less_table/$(target_symbol)__multi_symbol_BT_th($(th))_train.png")

        all_symbol_plot_train[th][target_symbol] = plt_train_mlr
    end


    begin
        println("Test")
        println("----------")
        sl_test, ss_test = get_signals(df_evv_test_multi_sb[!, "multi_equal_weight_sum"], th)
        plt_test_meq, tr_res_vec_test_meq = backtest_sica_2(sl_test, ss_test, df_test_map[target_symbol].WAP_Lag_200ms, df_test_map[target_symbol].WAP_Lag_0ms, df_test_map[target_symbol].timestamp, is_display=false, name=target_symbol)
        
        println("----------")
        sl_test, ss_test = get_signals(df_evv_test_multi_sb[!, "multi_lin_reg_weight_sum"], th)
        plt_test_mlr, tr_res_vec_test_mlr = backtest_sica_2(sl_test, ss_test, df_test_map[target_symbol].WAP_Lag_200ms, df_test_map[target_symbol].WAP_Lag_0ms, df_test_map[target_symbol].timestamp, is_display=false, name=target_symbol)
        
        println("----------")
        sl_test, ss_test = get_signals(df_evv_test_multi_sb[!, "multi_regime_weight_sum"], th)
        plt_test_mrg, tr_res_vec_test_mrg = backtest_sica_2(sl_test, ss_test, df_test_map[target_symbol].WAP_Lag_200ms, df_test_map[target_symbol].WAP_Lag_0ms, df_test_map[target_symbol].timestamp, is_display=false, name=target_symbol)
        
        # final_plt_test = plot(plt_test_meq, plt_test_mlr, plt_test_mrg, layout=(3, 1), plot_title="[$(target_symbol)] Multi Symbol Test - th: $th", size=(900, 900))        
        # savefig(final_plt_test, "./fig/multi_symbol_BT_less_table/$(target_symbol)__multi_symbol_BT_th($(th))_test.png")

        all_symbol_plot_test[th][target_symbol] = plt_test_mlr
    end
end

end     # symbolsë¡œ target_symbol ëŒë¦¬ëŠ” for loop


all_symbol_plot_train = plot(title="Multi Symbol Linear Regression")


for th in thv
    symbols_plot_train = plot([all_symbol_plot_train[th][sb] for sb in symbols]..., layout=(3, 2), plot_title="Multi Symbol Lin Reg th: $(th) [Train]", size=(2000, 1800))
    savefig(symbols_plot_train, "./fig/summary/multi_symbol_lin_reg_train_th($(th)).png")

    symbols_plot_test = plot([all_symbol_plot_test[th][sb] for sb in symbols]..., layout=(3, 2), plot_title="Multi Symbol Lin Reg th: $(th) [Test]", size=(2000, 1800))
    savefig(symbols_plot_test, "./fig/summary/multi_symbol_lin_reg_test_th($(th)).png")

end




# íˆ¬ë‘ ì™„ë£Œ
# 1. 48ê°œ í”¼ì³ equal weight, lr í•´ì„œ ë¹„êµ
# 2. ì‹¬ë³¼ ë³„ë¡œ í”¼ì³ ê¹ì•„ì„œ í•´ë³´ê¸°







# th = 0.05
# th = 0.1
# th = 0.33333
# th = 0.5
# th = 1.0
# th = 5.0


# begin
#     println("Train")
#     println("----------")
#     sl_train, ss_train = get_signals(df_evv_train_multi_sb[!, "multi_regime_weight_sum"], th)
#     plt_train_mrg, tr_res_vec_train_mrg = backtest_sica_2(sl_train, ss_train, df_train_map[target_symbol].WAP_Lag_200ms, df_train_map[target_symbol].WAP_Lag_0ms, df_train_map[target_symbol].timestamp, is_display=false)
#     display(plt_train_mrg)
#     # final_plt_train = plot(plt_train_eq, plt_train_lr, plt_train_rg, layout=(3, 1), plot_title="Train - th: $th", size=(900, 900))
# end


# begin
#     println("Test")
#     println("----------")
#     sl_test, ss_test = get_signals(df_evv_test_multi_sb[!, "multi_regime_weight_sum"], th)
#     plt_test_mrg, tr_res_vec_test_mrg = backtest_sica_2(sl_test, ss_test, df_test_map[target_symbol].WAP_Lag_200ms, df_test_map[target_symbol].WAP_Lag_0ms, df_test_map[target_symbol].timestamp, is_display=false)
#     display(plt_test_mrg)
#     # final_plt_train = plot(plt_train_eq, plt_train_lr, plt_train_rg, layout=(3, 1), plot_title="Train - th: $th", size=(900, 900))
# end



# ####################################
# # ìƒìœ„ 10ê°œ í”¼ì³ë§Œ ì‚¬ìš©í•´ì„œ ë‹¤ì‹œ í•´ë³´ì

# use_features = [
#     "BTCUSDT__ofi", "BTCUSDT__tv", "BTCUSDT__liquidation", "BTCUSDT__mark_dist", "BTCUSDT__index_dist", "BTCUSDT__tor", "BTCUSDT__aq", "BTCUSDT__vwap",
#     "SOLUSDT__vwap",
#     "SOLUSDT__mark_dist",
#     "SOLUSDT__liquidation",
#     # "BTCUSDT__liquidation",
#     # "BTCUSDT__ofi",
#     "DOGEUSDT__tor",
#     "XRPUSDT__tor",
#     "ETHUSDT__aq",
#     "SOLUSDT__aq",
#     "SOLUSDT__ofi",
# ]


# df_evv_train_filtered = df_evv_train_multi_sb[:, use_features]
# df_evv_test_filtered = df_evv_test_multi_sb[:, use_features]

# rg_info_map = get_rg_info_map(regime_ft_gen_map, df_train_map[target_symbol], df_test_map[target_symbol], df_evv_train_filtered)

# df_evv_train_multi_sb[!, "multi_regime_10_weight_sum"] = calc_multi_regime_based_er_vec(rg_info_map, df_evv_train_filtered, "mask_train")
# df_evv_test_multi_sb[!, "multi_regime_10_weight_sum"] = calc_multi_regime_based_er_vec(rg_info_map, df_evv_test_filtered, "mask_test")


# th = 0.05
# th = 0.1
# th = 0.33333
# th = 0.5
# th = 1.0
# th = 5.0

# thv = [0.05, 0.1, 0.33333, 0.5, 1.0]
# for th in thv
#     begin
#         println("Train")
#         println("----------")
#         sl_train, ss_train = get_signals(df_evv_train_multi_sb[!, "multi_regime_weight_sum"], th)
#         plt_train_mrg, tr_res_vec_train_mrg = backtest_sica_2(sl_train, ss_train, df_train_map[target_symbol].WAP_Lag_200ms, df_train_map[target_symbol].WAP_Lag_0ms, df_train_map[target_symbol].timestamp, is_display=false)
        
#         println("----------")
#         sl_train, ss_train = get_signals(df_evv_train_multi_sb[!, "multi_regime_10_weight_sum"], th)
#         plt_train_mrg_10, tr_res_vec_train_mrg_10 = backtest_sica_2(sl_train, ss_train, df_train_map[target_symbol].WAP_Lag_200ms, df_train_map[target_symbol].WAP_Lag_0ms, df_train_map[target_symbol].timestamp, is_display=false)
#         # display(plt_train_mrg)
#         final_plt_train = plot(plt_train_mrg, plt_train_mrg_10, layout=(2, 1), plot_title="[$(target_symbol)] Multi Symbol Train - th: $th", size=(900, 900))
#         # final_plt_train = plot(plt_train_mrg, layout=(1, 1), plot_title="[$(target_symbol)] Multi Symbol Train - th: $th", size=(900, 900))
#         savefig(final_plt_train, "./fig/$(target_symbol)__multi_symbol_BT_th($(th))_train2.png")
#     end


#     begin
#         println("Test")
#         println("----------")
#         sl_test, ss_test = get_signals(df_evv_test_multi_sb[!, "multi_regime_weight_sum"], th)
#         plt_test_mrg, tr_res_vec_test_mrg = backtest_sica_2(sl_test, ss_test, df_test_map[target_symbol].WAP_Lag_200ms, df_test_map[target_symbol].WAP_Lag_0ms, df_test_map[target_symbol].timestamp, is_display=false)
        
#         println("----------")
#         sl_test, ss_test = get_signals(df_evv_test_multi_sb[!, "multi_regime_10_weight_sum"], th)
#         plt_test_mrg_10, tr_res_vec_test_mrg_10 = backtest_sica_2(sl_test, ss_test, df_test_map[target_symbol].WAP_Lag_200ms, df_test_map[target_symbol].WAP_Lag_0ms, df_test_map[target_symbol].timestamp, is_display=false)
#         # display(plt_test_mrg)
#         final_plt_test = plot(plt_test_mrg, plt_test_mrg_10, layout=(2, 1), plot_title="[$(target_symbol)] Multi Symbol Test - th: $th", size=(900, 900))
#         # final_plt_test = plot(plt_test_mrg, layout=(1, 1), plot_title="[$(target_symbol)] Multi Symbol Test - th: $th", size=(900, 900))
#         savefig(final_plt_test, "./fig/$(target_symbol)__multi_symbol_BT_th($(th))_test2.png")
#     end
# end
# end



# ê²°ë¡ 
# 48ê°œ í”¼ì³ê°€ ì§êµëŠ” í•œë‹¤.
# ê·¼ë° í•©ì¹˜ë©´ ì„±ê³¼ê°€ ì˜ ë³„ë¡œì„ 
# ê·¸ëƒ¥ 1ê°œ ì¢…ëª©ì—ì„œ 8ê°œ ë½‘ì€ê±¸ë¡œ ì“°ì 
# 1ê°œ ì¢…ëª©ì—ì„œ ë‹¤ ì˜ í†µí•˜ëŠ”ì§€ë‚˜ í•œë²ˆ ë³´ì.... 










# ################################
# # ì„±ëŠ¥ì´ êµ¬ë¦¼. ì¼ë‹¨ 48ê°œ í”¼ì³ ì§êµì„±ì„ ì¢€ ë³´ì

# # ì¶”ê°€ íŒ¨í‚¤ì§€ import (ì§êµì„± ë¶„ì„ìš©)
# using LinearAlgebra
# using Statistics  
# using StatsBase
# using Printf

# # corr_analysis.jlì˜ í•¨ìˆ˜ë“¤ì„ ì—¬ê¸°ì— ì •ì˜
# """
# í”¼ì–´ìŠ¨ ìƒê´€ê´€ê³„ í–‰ë ¬ì„ ê³„ì‚°í•˜ê³  ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜
# """
# function analyze_feature_correlation(df_evv_train, df_evv_test, ft_names; title_suffix="")
#     println("\n" * "="^60)
#     println("Feature Correlation Analysis $(title_suffix)")
#     println("="^60)
    
#     # Train ë°ì´í„° ìƒê´€ê´€ê³„
#     train_data = Matrix(df_evv_train[!, ft_names])
#     # NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±°
#     valid_rows = .!any(isnan.(train_data), dims=2)[:]
#     train_clean = train_data[valid_rows, :]
    
#     corr_train = cor(train_clean)
    
#     # Test ë°ì´í„° ìƒê´€ê´€ê³„  
#     test_data = Matrix(df_evv_test[!, ft_names])
#     valid_rows_test = .!any(isnan.(test_data), dims=2)[:]
#     test_clean = test_data[valid_rows_test, :]
    
#     corr_test = cor(test_clean)
    
#     # ìƒê´€ê´€ê³„ í†µê³„ ì¶œë ¥
#     println("Train ë°ì´í„°:")
#     print_correlation_stats(corr_train, ft_names)
#     println("\nTest ë°ì´í„°:")
#     print_correlation_stats(corr_test, ft_names)
    
#     # íˆíŠ¸ë§µ ì‹œê°í™”
#     p1 = heatmap(corr_train, 
#                 xticks=(1:length(ft_names), ft_names), 
#                 yticks=(1:length(ft_names), ft_names),
#                 title="Train Correlation Matrix",
#                 color=:RdBu,
#                 clims=(-1, 1),
#                 xrotation=45)
    
#     p2 = heatmap(corr_test, 
#                 xticks=(1:length(ft_names), ft_names), 
#                 yticks=(1:length(ft_names), ft_names),
#                 title="Test Correlation Matrix", 
#                 color=:RdBu,
#                 clims=(-1, 1),
#                 xrotation=45)
    
#     plt_corr = plot(p1, p2, layout=(1, 2), size=(1400, 600))
#     display(plt_corr)
    
#     return corr_train, corr_test, plt_corr
# end

# """
# ìƒê´€ê´€ê³„ í†µê³„ ì¶œë ¥ í•¨ìˆ˜
# """
# function print_correlation_stats(corr_matrix, ft_names)
#     n = size(corr_matrix, 1)
    
#     # ëŒ€ê°ì„  ì œì™¸í•œ ìƒê´€ê³„ìˆ˜ë“¤ ì¶”ì¶œ
#     off_diag_corrs = []
#     for i in 1:n
#         for j in (i+1):n
#             push!(off_diag_corrs, abs(corr_matrix[i, j]))
#         end
#     end
    
#     println(@sprintf("  í‰ê·  ì ˆëŒ€ ìƒê´€ê³„ìˆ˜: %.4f", mean(off_diag_corrs)))
#     println(@sprintf("  ìµœëŒ€ ì ˆëŒ€ ìƒê´€ê³„ìˆ˜: %.4f", maximum(off_diag_corrs)))
#     println(@sprintf("  ìƒê´€ê³„ìˆ˜ > 0.5ì¸ ìŒ: %dê°œ (%.1f%%)", 
#              sum(off_diag_corrs .> 0.5), 
#              100 * sum(off_diag_corrs .> 0.5) / length(off_diag_corrs)))
#     println(@sprintf("  ìƒê´€ê³„ìˆ˜ > 0.7ì¸ ìŒ: %dê°œ (%.1f%%)", 
#              sum(off_diag_corrs .> 0.7), 
#              100 * sum(off_diag_corrs .> 0.7) / length(off_diag_corrs)))
    
#     # ë†’ì€ ìƒê´€ê´€ê³„ ìŒë“¤ ì¶œë ¥
#     high_corr_pairs = []
#     for i in 1:n
#         for j in (i+1):n
#             if abs(corr_matrix[i, j]) > 0.5
#                 push!(high_corr_pairs, (ft_names[i], ft_names[j], corr_matrix[i, j]))
#             end
#         end
#     end
    
#     if !isempty(high_corr_pairs)
#         println("  ë†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.5):")
#         for (ft1, ft2, corr_val) in sort(high_corr_pairs, by=x->abs(x[3]), rev=true)
#             println(@sprintf("    %s - %s: %.4f", ft1, ft2, corr_val))
#         end
#     end
# end

# """
# PCA ë¶„ì„ í•¨ìˆ˜
# """
# function analyze_feature_pca(df_evv_train, df_evv_test, ft_names; title_suffix="")
#     println("\n" * "="^60)
#     println("Principal Component Analysis (PCA) $(title_suffix)")
#     println("="^60)
    
#     # Train ë°ì´í„° ì¤€ë¹„
#     train_data = Matrix(df_evv_train[!, ft_names])
#     valid_rows = .!any(isnan.(train_data), dims=2)[:]
#     train_clean = train_data[valid_rows, :]
    
#     # ë°ì´í„° í‘œì¤€í™”
#     train_std = StatsBase.standardize(StatsBase.ZScoreTransform, train_clean, dims=1)
    
#     # PCA ìˆ˜í–‰ (SVD ì‚¬ìš©)
#     U, S, Vt = svd(train_std)
    
#     # ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨ ê³„ì‚°
#     explained_variance_ratio = (S .^ 2) ./ sum(S .^ 2)
#     cumulative_variance = cumsum(explained_variance_ratio)
    
#     # ê²°ê³¼ ì¶œë ¥
#     println("ì£¼ì„±ë¶„ë³„ ì„¤ëª…ëœ ë¶„ì‚°:")
#     for i in 1:min(length(S), 10)  # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ì¶œë ¥
#         println(@sprintf("  PC%d: %.4f (ëˆ„ì : %.4f)", 
#                 i, explained_variance_ratio[i], cumulative_variance[i]))
#     end
    
#     # 90% ë¶„ì‚°ì„ ì„¤ëª…í•˜ëŠ” ì£¼ì„±ë¶„ ê°œìˆ˜
#     n_components_90 = findfirst(cumulative_variance .>= 0.9)
#     n_components_95 = findfirst(cumulative_variance .>= 0.95)
#     println(@sprintf("\n90%% ë¶„ì‚° ì„¤ëª…ì— í•„ìš”í•œ ì£¼ì„±ë¶„: %dê°œ", n_components_90))
#     println(@sprintf("95%% ë¶„ì‚° ì„¤ëª…ì— í•„ìš”í•œ ì£¼ì„±ë¶„: %dê°œ", n_components_95))
    
#     # ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì˜ feature loading ì¶œë ¥
#     println("\nì²« ë²ˆì§¸ ì£¼ì„±ë¶„ (PC1) ë¡œë”© (ì ˆëŒ“ê°’ ê¸°ì¤€ ìƒìœ„ 10ê°œ):")
#     pc1_loadings = Vt[1, :]
#     loading_pairs = [(abs(pc1_loadings[i]), ft_names[i], pc1_loadings[i]) for i in 1:length(ft_names)]
#     sort!(loading_pairs, rev=true)
#     for i in 1:min(10, length(loading_pairs))
#         println(@sprintf("  %s: %.4f", loading_pairs[i][2], loading_pairs[i][3]))
#     end
    
#     # ì‹œê°í™”
#     p1 = plot(1:min(length(explained_variance_ratio), 15), 
#               explained_variance_ratio[1:min(length(explained_variance_ratio), 15)], 
#               seriestype=:bar, 
#               title="Explained Variance Ratio", 
#               xlabel="Principal Component", 
#               ylabel="Variance Ratio",
#               legend=false)
    
#     p2 = plot(1:min(length(cumulative_variance), 15), 
#               cumulative_variance[1:min(length(cumulative_variance), 15)], 
#               title="Cumulative Explained Variance", 
#               xlabel="Principal Component", 
#               ylabel="Cumulative Variance Ratio",
#               legend=false,
#               linewidth=2)
#     hline!(p2, [0.9, 0.95], linestyle=:dash, color=:red, alpha=0.5)
    
#     plt_pca = plot(p1, p2, layout=(1, 2), size=(1200, 400))
#     display(plt_pca)
    
#     return explained_variance_ratio, cumulative_variance, Vt, plt_pca
# end

# """
# ì§êµì„± ì¢…í•© ë¶„ì„ í•¨ìˆ˜
# """
# function comprehensive_orthogonality_analysis(df_evv_train, df_evv_test, ft_names)
#     println("\n" * "="^80)
#     println("í”¼ì²˜ ì§êµì„± ì¢…í•© ë¶„ì„")
#     println("="^80)
    
#     # 1. ìƒê´€ê´€ê³„ ë¶„ì„
#     corr_train, corr_test, plt_corr = analyze_feature_correlation(df_evv_train, df_evv_test, ft_names)
    
#     # 2. PCA ë¶„ì„
#     explained_var, cumulative_var, loadings, plt_pca = analyze_feature_pca(df_evv_train, df_evv_test, ft_names)
    
#     # 3. ì§êµì„± ì ìˆ˜ ê³„ì‚°
#     println("\n" * "="^60)
#     println("ì§êµì„± ì¢…í•© ì ìˆ˜")
#     println("="^60)
    
#     # Train ë°ì´í„° ê¸°ì¤€ ì§êµì„± ì ìˆ˜
#     n = length(ft_names)
#     off_diag_corrs = []
#     for i in 1:n
#         for j in (i+1):n
#             push!(off_diag_corrs, abs(corr_train[i, j]))
#         end
#     end
    
#     orthogonality_score = 1.0 - mean(off_diag_corrs)  # í‰ê·  ì ˆëŒ€ ìƒê´€ê³„ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ì§êµì„± ë†’ìŒ
#     pca_score = 1.0 - explained_var[1]  # ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ ë¶„ì‚°ì´ ì˜ ë¶„ì‚°ë¨
    
#     println(@sprintf("í‰ê·  ì ˆëŒ€ ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ ì§êµì„± ì ìˆ˜: %.4f (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì§êµ)", orthogonality_score))
#     println(@sprintf("PCA ê¸°ë°˜ ë¶„ì‚° ë¶„ì‚°ë„ ì ìˆ˜: %.4f (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¶„ì‚°ì´ ê³ ë¥´ê²Œ ë¶„í¬)", pca_score))
    
#     # 4. ê¶Œì¥ì‚¬í•­ ì¶œë ¥
#     println("\nê¶Œì¥ì‚¬í•­:")
#     if mean(off_diag_corrs) > 0.3
#         println("âš ï¸  í”¼ì²˜ë“¤ ê°„ ìƒê´€ê´€ê³„ê°€ ë†’ìŠµë‹ˆë‹¤. ì°¨ì› ì¶•ì†Œë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.")
#     else
#         println("âœ… í”¼ì²˜ë“¤ì´ ì ì ˆíˆ ë…ë¦½ì ì…ë‹ˆë‹¤.")
#     end
    
#     if explained_var[1] > 0.4
#         println("âš ï¸  ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì´ ë¶„ì‚°ì˜ 40% ì´ìƒì„ ì„¤ëª…í•©ë‹ˆë‹¤. í”¼ì²˜ ë‹¤ì–‘ì„±ì„ ëŠ˜ë ¤ë³´ì„¸ìš”.")
#     else
#         println("âœ… ë¶„ì‚°ì´ ì£¼ì„±ë¶„ë“¤ì— ê³ ë¥´ê²Œ ë¶„í¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
#     end
    
#     n_components_95 = findfirst(cumulative_var .>= 0.95)
#     efficiency = n_components_95 / length(ft_names)
#     if efficiency < 0.7
#         println(@sprintf("âœ… ì°¨ì› íš¨ìœ¨ì„±ì´ ì¢‹ìŠµë‹ˆë‹¤. %dê°œ í”¼ì²˜ë¡œ 95%% ë¶„ì‚° ì„¤ëª… ê°€ëŠ¥ (íš¨ìœ¨ì„±: %.2f)", n_components_95, efficiency))
#     else
#         println(@sprintf("âš ï¸  ì°¨ì› íš¨ìœ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. %dê°œ í”¼ì²˜ê°€ 95%% ë¶„ì‚° ì„¤ëª…ì— í•„ìš” (íš¨ìœ¨ì„±: %.2f)", n_components_95, efficiency))
#     end
    
#     return Dict(
#         "correlation_matrices" => (corr_train, corr_test),
#         "pca_results" => (explained_var, cumulative_var, loadings),
#         "orthogonality_score" => orthogonality_score,
#         "pca_score" => pca_score,
#         "plots" => (plt_corr, plt_pca)
#     )
# end

# # 48ê°œ í”¼ì³ ì§êµì„± ë¶„ì„ ì‹¤í–‰
# begin
#     println("\nğŸ” Multi-Symbol 48ê°œ í”¼ì³ ì§êµì„± ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
#     # timestampì™€ multi_regime_weight_sum ì œì™¸í•œ 48ê°œ í”¼ì³ ì¶”ì¶œ
#     all_columns = names(df_evv_train_multi_sb)
#     exclude_columns = ["timestamp", "multi_regime_weight_sum"]
#     multi_ft_names = [col for col in all_columns if !(col in exclude_columns)]
    
#     println("ë¶„ì„ ëŒ€ìƒ í”¼ì³ ìˆ˜: $(length(multi_ft_names))ê°œ")
#     println("í”¼ì³ ëª©ë¡:")
#     for (i, ft_name) in enumerate(multi_ft_names)
#         println("  $i. $ft_name")
#     end
    
#     # ì¢…í•© ì§êµì„± ë¶„ì„ ì‹¤í–‰
#     multi_orthogonality_results = comprehensive_orthogonality_analysis(
#         df_evv_train_multi_sb, df_evv_test_multi_sb, multi_ft_names)
    
#     println("\n" * "="^80)
#     println("ğŸ¯ Multi-Symbol 48ê°œ í”¼ì³ ë¶„ì„ ìš”ì•½")
#     println("="^80)
#     println(@sprintf("ì „ì²´ í”¼ì²˜ ìˆ˜: %dê°œ", length(multi_ft_names)))
#     println(@sprintf("ì§êµì„± ì ìˆ˜: %.4f", multi_orthogonality_results["orthogonality_score"]))
#     println(@sprintf("PCA ë¶„ì‚° ë¶„ì‚°ë„: %.4f", multi_orthogonality_results["pca_score"]))
    
#     # Symbolë³„, Featureë³„ ìƒê´€ê´€ê³„ íŒ¨í„´ ë¶„ì„
#     println("\nğŸ“Š Symbolë³„ í”¼ì³ ê·¸ë£¹ ë¶„ì„:")
#     symbol_groups = Dict()
#     for ft_name in multi_ft_names
#         if occursin("__", ft_name)
#             symbol, feature = split(ft_name, "__", limit=2)
#             if !haskey(symbol_groups, symbol)
#                 symbol_groups[symbol] = []
#             end
#             push!(symbol_groups[symbol], ft_name)
#         end
#     end
    
#     for (symbol, features) in symbol_groups
#         println("  $symbol: $(length(features))ê°œ í”¼ì³")
#     end
    
#     # ë™ì¼ í”¼ì³ íƒ€ì…ì˜ Symbolê°„ ìƒê´€ê´€ê³„ ë¶„ì„
#     println("\nğŸ“ˆ ë™ì¼ í”¼ì³ íƒ€ì…ì˜ Symbolê°„ ìƒê´€ê´€ê³„ ë¶„ì„:")
#     corr_train = multi_orthogonality_results["correlation_matrices"][1]
    
#     # í”¼ì³ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í•‘
#     feature_types = Dict()
#     for ft_name in multi_ft_names
#         if occursin("__", ft_name)
#             symbol, feature = split(ft_name, "__", limit=2)
#             if !haskey(feature_types, feature)
#                 feature_types[feature] = []
#             end
#             push!(feature_types[feature], ft_name)
#         end
#     end
    
#     for (feature_type, feature_list) in feature_types
#         if length(feature_list) > 1
#             # ë™ì¼ í”¼ì³ íƒ€ì… ë‚´ì—ì„œì˜ í‰ê·  ìƒê´€ê³„ìˆ˜ ê³„ì‚°
#             type_corrs = []
#             feature_indices = [findfirst(x -> x == ft, multi_ft_names) for ft in feature_list]
#             for i in 1:length(feature_indices)
#                 for j in (i+1):length(feature_indices)
#                     idx1, idx2 = feature_indices[i], feature_indices[j]
#                     push!(type_corrs, abs(corr_train[idx1, idx2]))
#                 end
#             end
#             if !isempty(type_corrs)
#                 println(@sprintf("  %s: í‰ê·  ì ˆëŒ€ ìƒê´€ê³„ìˆ˜ %.4f (Symbolê°„)", 
#                         feature_type, mean(type_corrs)))
#             end
#         end
#     end
# end












